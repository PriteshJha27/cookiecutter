import os
from typing import Any, List, Mapping, Optional, Dict
from langchain_core.callbacks.manager import CallbackManagerForLLMRun
from langchain_core.language_models.llms import LLM
from langchain_core.pydantic_v1 import Field, root_validator
import httpx
from dotenv import load_dotenv

class LlamaLLM(LLM):
    """Custom LLM wrapper for Llama API with authentication."""
    
    auth_url: str = Field(
        default="https://authbluesvega-vip.php.aesp.com/sso/signin",
        description="Authentication endpoint URL"
    )
    llama_url: str = Field(
        default="https://aidageniservices.qa.aesp.com/app/v1/opensource/models/llama3-7b-instruct/chat/completions",
        description="Llama API endpoint URL"
    )
    cert_path: str = Field(
        default="path/to/your/cert.pem",
        description="Path to certificate file"
    )
    user_id: str = Field(..., description="User ID for authentication")
    pwd: str = Field(..., description="Password for authentication")
    
    @root_validator()
    def validate_environment(cls, values: Dict) -> Dict:
        """Validate that required fields are present."""
        required_fields = ['auth_url', 'llama_url', 'user_id', 'pwd']
        for field in required_fields:
            if not values.get(field):
                raise ValueError(f"{field} must be provided")
        return values

    @property
    def _llm_type(self) -> str:
        """Return type of LLM."""
        return "llama_authenticated"

    async def _get_auth_cookie(self) -> str:
        """Get authentication cookie."""
        auth_payload = {
            "userId": self.user_id,
            "pwd": self.pwd
        }
        
        auth_headers = {
            "Content-Type": "application/x-www-form-urlencoded",
            "Accept": "*/*"
        }
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                self.auth_url,
                data=auth_payload,
                headers=auth_headers
            )
            
            if response.status_code != 200:
                raise Exception(f"Authentication failed: ({response.status_code}) {response.text}")
            
            auth_cookie = response.headers.get("Set-Cookie")
            if not auth_cookie:
                raise Exception("No cookie header found")
                
            return auth_cookie

    async def _llama_call(self, system_prompt: str, query: str) -> str:
        """Make authenticated call to Llama API."""
        auth_cookie = await self._get_auth_cookie()
        
        llama_payload = {
            "messages": [
                {
                    "content": system_prompt,
                    "role": "system"
                },
                {
                    "content": query,
                    "role": "user"
                }
            ],
            "model": "llama3-7b-instruct"
        }
        
        llama_headers = {
            "Content-Type": "application/json",
            "Accept": "application/json",
            "Cookie": auth_cookie
        }
        
        async with httpx.AsyncClient(verify=self.cert_path, timeout=60.0) as client:
            llama_response = await client.post(
                self.llama_url,
                json=llama_payload,
                headers=llama_headers
            )
            
            if llama_response.status_code != 200:
                raise Exception(f"Llama call failed: ({llama_response.status_code}) {llama_response.text}")
            
            return llama_response.json()

    async def _acall(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """Async call to the Llama API."""
        system_prompt = kwargs.get('system_prompt', "You are a helpful AI assistant.")
        output = await self._llama_call(system_prompt, prompt)
        return output.get("choices", [{}])[0].get("message", {}).get("content", "")

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """Synchronous call to the Llama API using asyncio."""
        import asyncio
        
        # Create event loop if it doesn't exist
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
        return loop.run_until_complete(
            self._acall(prompt, stop, run_manager, **kwargs)
        )

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """Get the identifying parameters."""
        return {
            "llama_url": self.llama_url,
            "model": "llama3-7b-instruct"
        }
